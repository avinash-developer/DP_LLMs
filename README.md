# PEFT Methods under Differential Privacy

Comparison of parameter-efficient fine-tuning (PEFT) methods for BERT-tiny on four GLUE classification tasks, evaluated with and without differential privacy (DP-SGD, ε = 8).

**Model:** `prajjwal1/bert-tiny`
**Tasks:** SST-2, QNLI, QQP, MNLI
**Privacy:** ε = 8 (DP-SGD) and ε = ∞ (non-private)

---

## Methods

| Method | Description |
|---|---|
| `full-finetuning` | All model parameters trained |
| `last-layer-finetuning` | Only the classification head trained |
| `lora` | Low-Rank Adaptation (PEFT) |
| `ia3` | IA³ — Infused Adapter by Inhibiting and Amplifying Inner Activations |
| `soft-prompt` | Learnable token embeddings prepended to input |
| `prefix` | Learned key-value pairs injected into each attention layer |
| `soft-prompt+lora` | LoRA weights + soft prompt tokens |
| `prefix+lora` | LoRA weights + learned attention-layer prefixes |

---

## Repository Structure

```
cispa/
    config.py         Hyperparameters: model name, batch size, epochs, DP delta, DP batch sizes
    data.py           GLUE dataset loading, tokenization, and DataLoader construction
    modeling.py       Model setup for each method; custom SoftPromptWrapper and PrefixWrapper classes
    training.py       Training loop for both standard SGD and DP-SGD, evaluation, and score extraction
    dp.py             Custom RDP-based DP-SGD: noise calibration, per-sample gradient clipping, accumulation
    cli.py            Argument parsing (--dataset, --method, --epsilon, --seed); writes temp_result.txt
    logging_utils.py  Configures stdout + file logging

main.py               Entry point — calls run_cli()
download_data.py      Pre-downloads all four GLUE datasets to ./glue_data_cache

run_param.sh          Runs one (method, dataset, epsilon) triple; saves to results/<id>.csv
run_method.sh         Loops over all datasets and epsilons for a given method
run_experiments.sh    Full sweep over all methods, datasets, and privacy settings
aggregate_results.sh  Concatenates all per-run CSVs into final_aggregated_results.csv

results.ipynb         Loads final_aggregated_results.csv and produces all plots
results/              Per-run CSV files: Privacy(Epsilon),Dataset,Method,Score,NumParams
plots/                Figures generated by results.ipynb (PNG + PDF)
final_aggregated_results.csv  Combined results for all runs
```

---

## Setup

**Requirements:** Python 3.10+, CUDA GPU recommended.

```bash
bash setup_cispa.sh
```

This creates a virtual environment at `./venv` and installs all dependencies from `requirements.txt`.

To pre-download GLUE datasets (avoids network calls during training):

```bash
venv/bin/python download_data.py
```

---

## Running Experiments

Activate the virtual environment or use the `venv/bin/python` path directly. All scripts use the venv automatically.

**Single run** — one method, one dataset, one privacy setting:

```bash
./run_param.sh <method> <dataset> <epsilon>

# Examples
./run_param.sh lora sst2 8      # DP, epsilon=8
./run_param.sh lora sst2 -1     # Non-private
./run_param.sh prefix+lora qnli 8
```

**All datasets and epsilons for one method:**

```bash
./run_method.sh <method>

# Example
./run_method.sh ia3
```

Available methods: `full-finetuning`, `last-layer-finetuning`, `lora`, `ia3`, `soft-prompt`, `prefix`, `soft-prompt+lora`, `prefix+lora`

**Full experiment sweep** (all methods × datasets × privacy settings):

```bash
./run_experiments.sh
```

Results are written to `results/<method>_<dataset>_EPS<epsilon>.csv` after each run.

**Aggregate results:**

```bash
bash aggregate_results.sh
```

Combines all per-run CSVs into `final_aggregated_results.csv`.

---

## Implementation Details

### Differential Privacy (`cispa/dp.py`)

Implements DP-SGD from scratch using Rényi Differential Privacy (RDP) accounting. The noise multiplier σ is calibrated to meet a target (ε, δ) guarantee at the end of training.

Each training step:
1. Splits the batch into microbatches of size 32
2. Computes per-sample gradients and clips each to L2 norm ≤ 1.0
3. Accumulates clipped gradients across the batch
4. Adds Gaussian noise scaled to σ · max_grad_norm
5. Applies the noisy gradient via AdamW

Key config values (`cispa/config.py`):

| Parameter | Value |
|---|---|
| DP delta | 1e-5 |
| DP batch size | 512 |
| Microbatch size | 32 |
| Max grad norm | 1.0 |

### PEFT Methods (`cispa/modeling.py`)

Standard methods (`lora`, `ia3`, `soft-prompt`, `prefix`) use the PEFT library via `get_peft_model`.

The two hybrid methods use custom wrappers:

- **`SoftPromptWrapper`** — prepends `num_tokens=20` learnable embeddings (dim 128) to the token embedding sequence before passing to BERT.
- **`PrefixWrapper`** — learns a `(num_tokens, num_layers × 2 × hidden_size)` embedding table, reshapes it into per-layer key-value pairs, and injects them as `past_key_values` into each BERT attention layer via `DynamicCache`.

For `prefix+lora`, LoRA adapters are applied first via `get_peft_model`, then the LoRA-modified BERT is wrapped with `PrefixWrapper`. This means both the attention weight updates (LoRA) and the attention context (prefix keys/values) are learned jointly.

### Trainable Parameters

| Method | # Params (2-class) | # Params (3-class MNLI) |
|---|---|---|
| last-layer-finetuning | 258 | 387 |
| ia3 | 2,050 | 2,179 |
| soft-prompt | 2,818 | 2,947 |
| prefix | 10,498 | 10,627 |
| lora | 8,450 | 8,579 |
| soft-prompt+lora | 11,010 | 11,139 |
| prefix+lora | 18,690 | 18,819 |
| full-finetuning | 4,386,178 | 4,386,307 |

---

## Results

Open `results.ipynb` in Jupyter to view:

- Per-dataset accuracy comparison (non-private vs. DP)
- Average accuracy across all tasks
- Privacy cost heatmap (accuracy drop when adding DP)
- Per-dataset line plots showing the privacy gap per method

Pre-generated plots are in `plots/`.

---

## Configuration

Edit `cispa/config.py` to change training settings:

```python
MODEL_NAME        = "prajjwal1/bert-tiny"
MAX_SEQ_LENGTH    = 128
BATCH_SIZE        = 2048
EPOCHS            = 20
MAX_GRAD_NORM     = 1.0

DP_DELTA          = 1e-5
DP_BATCH_SIZE     = 512
DP_MICROBATCH_SIZE = 32
```
